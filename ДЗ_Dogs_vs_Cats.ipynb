{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3eNh28pobqYC+YNeUczTg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BirukovAlex/neto_Python/blob/main/%D0%94%D0%97_Dogs_vs_Cats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nF3Fp2N3Y9g",
        "outputId": "02f352fd-2ed4-44c5-aea3-43115e218d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time"
      ],
      "metadata": {
        "id": "kJxlYVDE_fMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_files = glob('/content/drive/MyDrive/data/train/*.jpg')\n",
        "test_files = glob('/content/drive/MyDrive/data/test/*.jpg')\n",
        "\n",
        "print(f\" Найдено тренировочных изображений: {len(train_files)}\")\n",
        "print(f\" Найдено тестовых изображений: {len(test_files)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yIwflMOaVGo",
        "outputId": "21ef34d0-f041-402e-ea6b-04665970f74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Найдено тренировочных изображений: 25001\n",
            " Найдено тестовых изображений: 12500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_files, val_files = train_test_split(\n",
        "    train_files,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(f\" Обучающая выборка: {len(train_files)} изображений\")\n",
        "print(f\" Валидационная выборка: {len(val_files)} изображений\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIG-BTHfamAL",
        "outputId": "92df2278-ec66-4a44-baa4-aa9d8b1f8538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Обучающая выборка: 20000 изображений\n",
            " Валидационная выборка: 5001 изображений\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ajThOYmZ_tuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ДООБУЧЕНИЕ НА БОЛЬШЕЙ ВЫБОРКЕ БЕЗ ПЕРЕГРУЗКИ ПАМЯТИ\")\n",
        "\n",
        "# Увеличиваем выборку но оставляем маленький batch size\n",
        "print(\"Создание расширенной выборки...\")\n",
        "\n",
        "def create_larger_dataset(files, batch_size=16):  # Уменьшаем batch_size для памяти\n",
        "    def load_image(path):\n",
        "        import cv2\n",
        "        img = cv2.imread(path)\n",
        "        if img is None:\n",
        "            return None\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, IMG_SIZE)\n",
        "        img = preprocess_input(img.astype(np.float32))\n",
        "        return img\n",
        "\n",
        "    def generator():\n",
        "        for path in files:\n",
        "            img = load_image(path)\n",
        "            if img is not None:\n",
        "                label = 1.0 if 'dog' in os.path.basename(path).lower() else 0.0\n",
        "                yield img, label\n",
        "\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(*IMG_SIZE, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(), dtype=tf.float32)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    dataset = dataset.shuffle(min(1000, len(files)))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Берем больше данных но с меньшим batch_size\n",
        "print(\"Используем 4000 train и 1000 val изображений...\")\n",
        "larger_train_dataset = create_larger_dataset(train_files[:4000])\n",
        "larger_val_dataset = create_larger_dataset(val_files[:1000])\n",
        "\n",
        "print(\"Быстрое дообучение на расширенной выборке...\")\n",
        "\n",
        "# Быстрое обучение на 5 эпохах\n",
        "history_final = model.fit(\n",
        "    larger_train_dataset,\n",
        "    epochs=5,\n",
        "    validation_data=larger_val_dataset,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Финальная оценка\n",
        "final_val_loss, final_val_accuracy = model.evaluate(larger_val_dataset, verbose=0)\n",
        "print(f\"\\nФИНАЛЬНЫЕ РЕЗУЛЬТАТЫ НА 4000/1000:\")\n",
        "print(f\"Val Loss: {final_val_loss:.4f}\")\n",
        "print(f\"Val Accuracy: {final_val_accuracy:.4f}\")\n",
        "\n",
        "# Предсказания на всех тестовых данных\n",
        "print(\"\\nФинальные предсказания на тестовом наборе...\")\n",
        "\n",
        "def create_test_dataset_generator(files, batch_size=16):\n",
        "    def load_test_image(path):\n",
        "        import cv2\n",
        "        img = cv2.imread(path)\n",
        "        if img is None:\n",
        "            return None\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, IMG_SIZE)\n",
        "        img = preprocess_input(img.astype(np.float32))\n",
        "        return img\n",
        "\n",
        "    def test_generator():\n",
        "        for path in files:\n",
        "            img = load_test_image(path)\n",
        "            if img is not None:\n",
        "                yield img\n",
        "\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        test_generator,\n",
        "        output_signature=tf.TensorSpec(shape=(*IMG_SIZE, 3), dtype=tf.float32)\n",
        "    )\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Предсказания на всех тестовых файлах\n",
        "test_dataset_final = create_test_dataset_generator(test_files)\n",
        "test_predictions_final = model.predict(test_dataset_final, verbose=1)\n",
        "\n",
        "print(f\"Сгенерировано предсказаний: {len(test_predictions_final)}\")\n",
        "\n",
        "# Создаем финальный файл\n",
        "print(\"Создание финального submission.csv...\")\n",
        "with open('submission_final.csv', 'w') as f:\n",
        "    f.write('id,label\\n')\n",
        "    for i, path in enumerate(test_files):\n",
        "        if i < len(test_predictions_final):\n",
        "            file_id = re.search(r'(\\d+)\\.jpg$', path).group(1)\n",
        "            f.write(f'{file_id},{test_predictions_final[i][0]:.6f}\\n')\n",
        "\n",
        "print(\"✅ Файл submission_final.csv создан!\")\n",
        "print(f\"✅ Обучение на {4000} тренировочных изображениях\")\n",
        "print(f\"✅ Валидация на {1000} изображениях\")\n",
        "print(f\"✅ Тестирование на {len(test_files)} изображениях\")\n",
        "print(f\"✅ Финальный Log Loss: {final_val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nТеперь submission должен быть принят Kaggle!\")import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time"
      ],
      "metadata": {
        "id": "um15LBtGaHKN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}